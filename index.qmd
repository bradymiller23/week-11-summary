---
title: "Weekly Summary Template"
author: "Brady Miller"
title-block-banner: true
title-block-style: default
toc: true
format: html
# format: pdf
---

---


## Thursday, March 30



::: {.callout-important}
## TIL

Include a _very brief_ summary of what you learnt in this class here. 

Today, I learnt the following concepts in class:

1. Item 1
1. Item 2
1. Item 3
:::


```{r}
library(tidyverse)
library(torch)
library(glmnet)
library(caret)
library(dplyr)
library(tidyr)
library(dplyr)
library(nnet)
library(mlbench)
library(class)
library(rpart.plot)
```

#### Plotting boundaries using decision trees

Creating plot of points
```{r}
n <- 200
X <- t(replicate(n, 2 * runif(2) -1))
y <- apply(X, 1, ex1) %>% as.factor()
col <- ifelse(y == 0, 'blue', 'red')
df <- data.frame(y = y, x1 = X[,1], x2 = X[,2])
plot(df$x1, df$x2, col = col, pch = 19)
```

Creating the logistic function and the predictions on the model that we will
graph
```{r}
f_logistic = \(x) predict(model, data.frame(x1=x[,1], x2 = x[,2]), 
                          type = 'response')

xnew <- cbind(
  x1 = rep(seq(-1.1,1.1,length.out=50),50),
  x2 = rep(seq(-1.1,1.1,length.out=50),each = 50)
)

plt <- function(f, x) {
  plot(x[,1], x[,2], col=ifelse(f(x) < 0.5, 'blue', 'red'), pch =22)
  points(df$x1, df$x2, col=ifelse(y=='0', 'blue', 'red'), pch =22)
}
```


Creating an overview function that creates a table of the predictions vs actual
values 
```{r}
overview <- function(f){
  11
  predicted <- ifelse(f(df[,-1])<0.5, 0, 1)
  actual <- df[,1]
  table(predicted, actual)
}
```

```{r}
plt(f_logistic, xnew)
overview(f_logistic)
```
This plot demonstrates the linear regression would not be a good choice for this
data set as many of the points are misclassified. This is also shown in the 
confusion matrix, as 70 of the 99 points that are actually blue are classified/
predicted as red, while 21 of the 101 points that are actually red are 
predicted as blue.


Creating a decision tree model that we will use to predict the class of the
points.
```{r}
dtree <- rpart(y ~ x1 + x2, df, method = 'class')
rpart.plot(dtree)

f_dtree <- \(x) as.numeric(predict(dtree, data.frame(x1 = x[,1], x2=x[,2]), 
                                   type = 'class')) - 1
plt(f_dtree, xnew)
```
This plot does a much better job in correctly classifying the points

```{r}
overview(f_dtree)
```
Based on the confusion matrix, it actually perfectly predicts the values.


#### Creating Neural Network with 1 hidden layer
```{r}
# neural network with 1 hidden layer
module <- nn_module(
  intialize = function() {
    self$f <- nn_linear(2,20)
    self$g <- nn_linear(20,1)
    self$a <- nn_relu()
    self$s <- nn_sigmoid()
  },
  forward = function(x) {
    x %>%
      self$f() %>%
      self$a() %>%
      self$g() %>%
      self$a() %>%
      self$s() 
  }
)
```



```{r}
# pre-process data
X_tensor <- torch_tensor(df[, -1] %>% as.matrix(), dtype = torch_float())
y_tensor <- torch_tensor(cbind(df[, 1] %>% as.numeric() - 1), dtype = torch_float())
```

```{r}
Loss <- function(x, y, model){
  nn_bce_loss()(model(x),y)
}
```

```{r}
F <- module()
Loss(X_tensor, y_tensor, module)


F <- module()
optimizer <- optim_adam(F$parameters, lr = 0.05)
epochs <- 1000

for(i in 1:epochs){
  loss <- Loss(X_tensor, y_tensor, F)
  
  optimizer$zero_grad()
  loss$backward()
  optimizer$step()
  
  if (i < 10 || i %% 100 == 0) {
    cat(sprintf("Epoch: %d, Loss: %.4f\n", i, loss$item()))
  }
}


f_nn = \(x) as_array(F(torch_tensor(x %>% as.matrix(), dtype = torch_float())))

plot(f_nn, df_new)

overview(f_logistic)
overview(f_dtree)
overview(f_svm)
```


#### Neural Network with 2 hidden layer

```{r}
Xnew <- cbind(
  x1 = rep(seq(-1, 1, length.out = 50), 50),
  x2 = rep(seq(-1, 1, length.out = 50), each = 50)
)
df_new = data.frame(x1=Xnew[,1], x2=Xnew[,2])

p <- 2
q <- 20
q1 <- 100
q2 <- 20

hh2_module <- nn_module(
  initialize = function() {
    self$f <- nn_linear(p,q1)
    self$g <- nn_linear(q1,q2)
    self$h <- nn_linear(q2,1)
    self$a <- nn_relu()
    self$s <- nn_sigmoid()
  },
  forward = function(x) {
    x %>%
      self$f() %>%
      self$a() %>%
      self$g() %>%
      self$a() %>%
      self$h() %>%
      self$s()
  }
)




F <- hh2_module()
optimizer <- optim_adam(F$parameters, lr = 0.05)
epochs <- 1000

for(i in 1:epochs){
  loss <- Loss(X_tensor, y_tensor, F)
  
  optimizer$zero_grad()
  loss$backward()
  optimizer$step()
  
  if (i < 10 || i %% 100 == 0) {
    cat(sprintf("Epoch: %d, Loss: %.4f\n", i, loss$item()))
  }
}


f_nn = \(x) as_array(F(torch_tensor(x %>% as.matrix(), dtype = torch_float())))
overview(f_logistic)
overview()

#f <- classifier(df, 'nn')
#plt(f, df_new)
#overview(df)
```





#### Regression with Neural Networks

This code chunk is creating a graph of points in a sinusodial like curve which 
we will use to demonstrate the various regression techniques on and see which 
ones work the best
```{r}
generate_data <- function(n, noise = 0.1) {
  x <- seq(1*pi, 2*pi, length.out = n)
  y <- exp(x) * (sin(150/x) + rnorm(n, 0, noise))
  data.frame(x = x, y = y)
}

df <- generate_data(200, noise = 0.1)
plot(df$x, df$y, pch=19)
```

```{r}
x_new <- seq(0.9 * pi, 2.1*pi,  length.out = 1000)
df_new <- data.frame(x = x_new)

plt_reg <- function (f, x){
  ynew <- f(x)
  ylim <- range(c(ynew, df$y))
  ylim[1] <- max(c(-800, ylim[1]))
  ylim[2] <- min(c(250, ylim[2]))
  xlim <-range(x)
  plot(df$x, df$y, pch = 22, col = 'red', xlim=xlim, ylim = ylim)
  points(x[,1], ynew, pch=22, type='l')
}  
```

Linear regression on that plot
```{r}
f_lm = \(x)
    lm(y ~ x, df) %>%
    predict(., x)
plt_reg(f_lm, df_new)
```



Polynomial regression on that plot
```{r}
f_polynomial = \(x)
    lm(y ~ x + I(x^2) + I(x^3) + I(x^5), df) %>%
    predict(., x)
plt_reg(f_polynomial, df_new)
```


ree
```{r}
f_dtree = \(x)
    rpart(y ~ x, df) %>%
    predict(., x)
plt_reg(f_dtree, df_new)
```


Support Vector Machine
```{r}
f_svm = \(x)
    svm(y ~ x, df, kernel = 'radial') %>%
    predict(., x)
plt_reg(f_svm, df_new)
```

Neural Network
```{r}
reg_module <- nn_module(
  initialize = function() {
    self$f <- nn_linear(1,20)
    self$g <- nn_linear(20,100)
    self$h <- nn_linear(100,1)
    self$a <- nn_relu()
    self$s <- nn_sigmoid()
  },
  forward = function(x) {
    x %>%
      self$f() %>%
      self$a() %>%
      self$g() %>%
      self$a() %>%
      self$h() %>%
      self$s()
  }
)

f_nn <- function(x){
  F <- reg_module()
  X_tensor <- torch_tensor(df$x %>% as.matrix(), dtype=torch_float())
  y_tensor <- torch_tensor(cbind(df$y), dtype=torch_float())
  optimizer <- optim_adam(F$parameters, lr = 0.05)
  epochs <- 2000
  
  for(i in epochs){
    loss <- nn_mse_loss()(F(X_tensor), y_tensor)
    optimizer$zero_grad()
    loss$backward()
    optimizer$step()
  }
  return(as_array(F(torch_tensor(x %>% as.matrix(), dtype=torch_float()))))
} 

plt_reg(f_nn, df_new)
```




Getting rid of sigmoid layer in nn creates neural network regression instead of
nn classification


Can switch from relu() to sigmoid() to make a smoother curve (still for regression
but is being used as activation)
}
```