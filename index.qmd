---
title: "Weekly Summary Template"
author: "Brady Miller"
title-block-banner: true
title-block-style: default
toc: true
format: html
# format: pdf
---

---


## Thursday, March 30



::: {.callout-important}
## TIL

Include a _very brief_ summary of what you learnt in this class here. 

Today, I learnt the following concepts in class:

1. Item 1
1. Item 2
1. Item 3
:::


```{r}
library(tidyverse)
library(torch)
library(glmnet)
library(caret)
library(dplyr)
library(tidyr)
library(dplyr)
library(nnet)
```


```{r}
ex1 <- \(x) ifelse(
  sign(x[1] * x[2]) +0.01 * rnorm(1) <= 0, 0, 1)

n <- 200
X <- t(replicate(n, 2 * runif(2) -1))
y <- apply(X, 1, ex1) %>% as.factor()
col <- ifelse(y == 0, 'blue', 'red')
df <- data.frame(y = y, x1 = X[,1], x2 = X[,2])
plot(df$x1, df$x2, col = col, pch = 19)
```


```{r}
# neural network with 1 vs 2 hidden layers
module <- nn_module(
  intialize = function() {
    self$f <- nn_linear(2,20)
    self$g <- nn_linear(20,1)
    self$a <- nn_relu()
    self$s <- nn_sigmoid()
  },
  forward = function(x) {
    x %>%
      self$f() %>%
      self$a() %>%
      self$g() %>%
      self$a() %>%
      self$s() 
  }
)

```



```{r}
X_tensor <- torch_tensor(df[, -1] %>% as.matrix(), dtype = torch_float())
y_tensor <- torch_tensor(cbind(df[, 1] %>% as.numeric() - 1), dtype = torch_float())
```

```{r}
Loss <- function(x, y, model){}

  
```

```{r}
###
###
###
###
###
###
plot(f_nn, df_new)

overview(f_logistic)
overview(f_dtree)
overview(f_svm)
```


```{r}
classifier <-
  function(train, type = 'nn', ...) {
    if(type == 'logistic'){
      ######
    }
    elseif(type == 'rpart'){
      ########
    }
    elseif(type == 'svm'){
      ####
    }
    elseif(type == 'nn'){
      #########
    }
  }

```


#### Regression with Neural Networks

```{r}
generate_data <- function(n, noise = 0.1) {
  x <- seq(1*pi, 2*pi, length.out = n)
  y <- exp(x) * (sin(150/x) + rnorm(n, 0, noise))
  data.frame(x = x, y = y)
}

df <- generate_data(200, noise = 0.1)
plot(df$x, df$y, pch=19)
```
```{r}
x_new <- seq(0.9 * pi, 2.1*pi,  length.out = 1000)
df_new <- data.frame(x = x_new)

plt_reg <- function (f, x,...){
  ynew <- f(x)
  ylim <- range(c(ynew, df$y))
  ylim[1] <- max(c(-800, ylim[1]))
  ylim[2] <- min(c(250, ylim[2]))
  xlim <-range(x)
  plt(df$x, df$y, pch = 22, col = 'red', xlim=xlim, ylim = ylim)
  
```


Getting rid of sigmoid layer in nn creates neural network regression instead of
nn classification

#plt_reg(f_nn, df_new)

Can switch from relu() to sigmoid() to make a smoother curve (still for regression
but is being used as activation)
}
```
